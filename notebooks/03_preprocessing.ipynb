{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-01T14:17:38.382644Z",
     "start_time": "2026-01-01T14:17:38.172778Z"
    }
   },
   "source": [
    "# ---------------------------------------------------------\n",
    "# Task 1: Setup & Text Normalization\n",
    "# ---------------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "# 1. Load the Parsed Data\n",
    "# Ensure the path points to your saved CSV from Phase 1\n",
    "input_path = Path(r'../data/processed/parsed_data.csv')\n",
    "df = pd.read_csv(input_path, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"Loaded {len(df)} reviews.\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# DEFINITION: Persian Text Normalizer\n",
    "# ---------------------------------------------------------\n",
    "def normalize_persian(text):\n",
    "    \"\"\"\n",
    "    Standardizes Persian characters:\n",
    "    1. Converts Arabic 'ÙŠ' and 'Ùƒ' to Persian 'ÛŒ' and 'Ú©'.\n",
    "    2. Removes Arabic diacritics (fatha, damma, etc.).\n",
    "    3. Replaces half-spaces (ZWNJ) with standard spaces (optional, usually better for grouping).\n",
    "    4. Strips extra whitespace.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    \n",
    "    # Character Translation Map\n",
    "    translations = str.maketrans({\n",
    "        'ÙŠ': 'ÛŒ',\n",
    "        'Ùƒ': 'Ú©',\n",
    "        'Ø©': 'Ù‡',\n",
    "        'Ø¢': 'Ø§',\n",
    "        'Ø¥': 'Ø§',\n",
    "        'Ø£': 'Ø§',\n",
    "        'Ø¤': 'Ùˆ',\n",
    "        '\\u200c': ' ',  # Convert Zero-Width Non-Joiner (half-space) to space\n",
    "    })\n",
    "    \n",
    "    text = text.translate(translations)\n",
    "    \n",
    "    # Remove Arabic Diacritics (Fatha, Kasra, etc.)\n",
    "    text = re.sub(r'[\\u064B-\\u065F]', '', text)\n",
    "    \n",
    "    # Collapse multiple spaces into one\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# EXECUTION: Normalize Text Columns\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# We apply this to Name, Dept, Course, and Comments\n",
    "text_cols = ['professor_name_raw', 'department', 'course_name', 'comment_text']\n",
    "\n",
    "for col in text_cols:\n",
    "    # Create new 'clean_' columns so we preserve the original data for comparison\n",
    "    df[f'clean_{col}'] = df[col].apply(normalize_persian)\n",
    "\n",
    "print(\"Text Normalization Complete.\")\n",
    "\n",
    "# Show a sample of the changes\n",
    "display(df[['professor_name_raw', 'clean_professor_name_raw']].head())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4548 reviews.\n",
      "Text Normalization Complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  professor_name_raw clean_professor_name_raw\n",
       "0    Ø³ÛŒØ¯ Ù…Ø­Ù…Ø¯ Ø´Ù‡Ø±ØªØ§Ø´          Ø³ÛŒØ¯ Ù…Ø­Ù…Ø¯ Ø´Ù‡Ø±ØªØ§Ø´\n",
       "1   Ø¯Ú©ØªØ± Ù‡Ø§Ø¬Ø± ÙÙ„Ø§Ø­ØªÛŒ         Ø¯Ú©ØªØ± Ù‡Ø§Ø¬Ø± ÙÙ„Ø§Ø­ØªÛŒ\n",
       "2          Ø¹Ù„Ù… Ø§Ù„Ù‡Ø¯ÛŒ                Ø¹Ù„Ù… Ø§Ù„Ù‡Ø¯ÛŒ\n",
       "3          Ø­Ø¬Øª Ù‚Ø§Ø³Ù…ÛŒ                Ø­Ø¬Øª Ù‚Ø§Ø³Ù…ÛŒ\n",
       "4       ÙØ±ÛŒØ¨Ø±Ø² Ù‚Ø§Ø³Ù…ÛŒ             ÙØ±ÛŒØ¨Ø±Ø² Ù‚Ø§Ø³Ù…ÛŒ"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>professor_name_raw</th>\n",
       "      <th>clean_professor_name_raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ø³ÛŒØ¯ Ù…Ø­Ù…Ø¯ Ø´Ù‡Ø±ØªØ§Ø´</td>\n",
       "      <td>Ø³ÛŒØ¯ Ù…Ø­Ù…Ø¯ Ø´Ù‡Ø±ØªØ§Ø´</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ø¯Ú©ØªØ± Ù‡Ø§Ø¬Ø± ÙÙ„Ø§Ø­ØªÛŒ</td>\n",
       "      <td>Ø¯Ú©ØªØ± Ù‡Ø§Ø¬Ø± ÙÙ„Ø§Ø­ØªÛŒ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ø¹Ù„Ù… Ø§Ù„Ù‡Ø¯ÛŒ</td>\n",
       "      <td>Ø¹Ù„Ù… Ø§Ù„Ù‡Ø¯ÛŒ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ø­Ø¬Øª Ù‚Ø§Ø³Ù…ÛŒ</td>\n",
       "      <td>Ø­Ø¬Øª Ù‚Ø§Ø³Ù…ÛŒ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ÙØ±ÛŒØ¨Ø±Ø² Ù‚Ø§Ø³Ù…ÛŒ</td>\n",
       "      <td>ÙØ±ÛŒØ¨Ø±Ø² Ù‚Ø§Ø³Ù…ÛŒ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T14:25:09.949365Z",
     "start_time": "2026-01-01T14:25:09.935379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Filter for rows where the cleaning actually changed something\n",
    "changes = df[df['professor_name_raw'] != df['clean_professor_name_raw']]\n",
    "\n",
    "print(f\"Total rows changed: {len(changes)}\")\n",
    "\n",
    "if len(changes) > 0:\n",
    "    print(\"\\nSample Differences (Hidden characters revealed):\")\n",
    "    # We display the length to prove they are different strings\n",
    "    sample = changes[['professor_name_raw', 'clean_professor_name_raw']].head(5).copy()\n",
    "    sample['Raw Length'] = sample['professor_name_raw'].apply(len)\n",
    "    sample['Clean Length'] = sample['clean_professor_name_raw'].apply(len)\n",
    "    display(sample)\n",
    "else:\n",
    "    print(\"No changes found (Data might have been clean already).\")"
   ],
   "id": "8884e9e8d125c54c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows changed: 246\n",
      "\n",
      "Sample Differences (Hidden characters revealed):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "    professor_name_raw clean_professor_name_raw  Raw Length  Clean Length\n",
       "19        Ø³Ø§Ø³Ø§Ù† Ø¢Ø³ÛŒØ§ÛŒÛŒ             Ø³Ø§Ø³Ø§Ù† Ø§Ø³ÛŒØ§ÛŒÛŒ          12            12\n",
       "26  Ø³ÛŒØ¯ Ø­Ø³Ù† Ù‡Ø§Ø´Ù… Ø¢Ø¨Ø§Ø¯ÛŒ       Ø³ÛŒØ¯ Ø­Ø³Ù† Ù‡Ø§Ø´Ù… Ø§Ø¨Ø§Ø¯ÛŒ          18            18\n",
       "42    Ù…Ø¬ÛŒØ¯ Ø§ÛŒÙ„Ú†ÛŒ Ù‚Ø²Ø§Ø¢Ù†         Ù…Ø¬ÛŒØ¯ Ø§ÛŒÙ„Ú†ÛŒ Ù‚Ø²Ø§Ø§Ù†          16            16\n",
       "48      Ø§Ø³Ø¯Ø§Ù„Ù‡ Ø¢Ù‚Ø§Ø¬Ø§Ù†ÛŒ           Ø§Ø³Ø¯Ø§Ù„Ù‡ Ø§Ù‚Ø§Ø¬Ø§Ù†ÛŒ          14            14\n",
       "87    Ø¢Ø³ÛŒÙ‡ Ø³Ø§Ø¯Ø§Øª Ú©Ø§Ø¸Ù…ÛŒ         Ø§Ø³ÛŒÙ‡ Ø³Ø§Ø¯Ø§Øª Ú©Ø§Ø¸Ù…ÛŒ          16            16"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>professor_name_raw</th>\n",
       "      <th>clean_professor_name_raw</th>\n",
       "      <th>Raw Length</th>\n",
       "      <th>Clean Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Ø³Ø§Ø³Ø§Ù† Ø¢Ø³ÛŒØ§ÛŒÛŒ</td>\n",
       "      <td>Ø³Ø§Ø³Ø§Ù† Ø§Ø³ÛŒØ§ÛŒÛŒ</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Ø³ÛŒØ¯ Ø­Ø³Ù† Ù‡Ø§Ø´Ù… Ø¢Ø¨Ø§Ø¯ÛŒ</td>\n",
       "      <td>Ø³ÛŒØ¯ Ø­Ø³Ù† Ù‡Ø§Ø´Ù… Ø§Ø¨Ø§Ø¯ÛŒ</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Ù…Ø¬ÛŒØ¯ Ø§ÛŒÙ„Ú†ÛŒ Ù‚Ø²Ø§Ø¢Ù†</td>\n",
       "      <td>Ù…Ø¬ÛŒØ¯ Ø§ÛŒÙ„Ú†ÛŒ Ù‚Ø²Ø§Ø§Ù†</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Ø§Ø³Ø¯Ø§Ù„Ù‡ Ø¢Ù‚Ø§Ø¬Ø§Ù†ÛŒ</td>\n",
       "      <td>Ø§Ø³Ø¯Ø§Ù„Ù‡ Ø§Ù‚Ø§Ø¬Ø§Ù†ÛŒ</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Ø¢Ø³ÛŒÙ‡ Ø³Ø§Ø¯Ø§Øª Ú©Ø§Ø¸Ù…ÛŒ</td>\n",
       "      <td>Ø§Ø³ÛŒÙ‡ Ø³Ø§Ø¯Ø§Øª Ú©Ø§Ø¸Ù…ÛŒ</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T14:33:24.351834Z",
     "start_time": "2026-01-01T14:33:24.340729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------------------------------------\n",
    "# NORMALIZATION TEST SUITE (Corrected)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def run_normalization_tests():\n",
    "    \n",
    "    test_cases = [\n",
    "        # Case 1: Arabic Y and K\n",
    "        (\"Ø¹Ù„ÙŠ ÙƒØ±ÛŒÙ…ÛŒ\", \"Ø¹Ù„ÛŒ Ú©Ø±ÛŒÙ…ÛŒ\", \"Arabic 'Y' and 'K' conversion\"),\n",
    "        \n",
    "        # Case 2: ZWNJ -> Space (Standard behavior)\n",
    "        (\"Ø¯Ø§Ù†Ø´Ú©Ø¯Ù‡\\u200cÙÙ†ÛŒ\", \"Ø¯Ø§Ù†Ø´Ú©Ø¯Ù‡ ÙÙ†ÛŒ\", \"Zero-Width Non-Joiner removal\"),\n",
    "        \n",
    "        # Case 3: Diacritics\n",
    "        (\"Ù…ÙØ­ÙÙ…Ù‘Ø¯\", \"Ù…Ø­Ù…Ø¯\", \"Arabic Diacritics stripping\"),\n",
    "        \n",
    "        # Case 4: Whitespace\n",
    "        (\"   Ø¯Ú©ØªØ±    Ø±Ø¶Ø§ÛŒÛŒ  \\n\", \"Ø¯Ú©ØªØ± Ø±Ø¶Ø§ÛŒÛŒ\", \"Whitespace cleaning\"),\n",
    "        \n",
    "        # Case 5: Mixed Edge Case\n",
    "        # CORRECTED EXPECTATION: ZWNJ becomes a space, matching the output we want\n",
    "        (\"  Ø¢Ù‚Ø§ÙŠ Ù…ÙØ­ÙÙ…Ù‘Ø¯\\u200cØ±Ø¶Ø§   ÙƒØ§Ø¸Ù…ÛŒ  \", \"Ø§Ù‚Ø§ÛŒ Ù…Ø­Ù…Ø¯ Ø±Ø¶Ø§ Ú©Ø§Ø¸Ù…ÛŒ\", \"Mixed Edge Case\") \n",
    "    ]\n",
    "\n",
    "    passed_count = 0\n",
    "    \n",
    "    for i, (raw, expected, desc) in enumerate(test_cases, 1):\n",
    "        result = normalize_persian(raw)\n",
    "        \n",
    "        if result == expected:\n",
    "            print(f\"   TEST {i}: {desc} -> PASS\")\n",
    "            passed_count += 1\n",
    "        else:\n",
    "            print(f\"   TEST {i}: {desc} -> FAIL\")\n",
    "            print(f\"   Input:    '{raw}'\")\n",
    "            print(f\"   Expected: '{expected}'\")\n",
    "            print(f\"   Got:      '{result}'\")\n",
    "            \n",
    "    if passed_count == len(test_cases):\n",
    "        print(\"\\n   ALL SYSTEM TESTS PASSED.\")\n",
    "\n",
    "# Run the corrected tests\n",
    "run_normalization_tests()"
   ],
   "id": "6dbe5e23db119295",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   TEST 1: Arabic 'Y' and 'K' conversion -> PASS\n",
      "   TEST 2: Zero-Width Non-Joiner removal -> PASS\n",
      "   TEST 3: Arabic Diacritics stripping -> PASS\n",
      "   TEST 4: Whitespace cleaning -> PASS\n",
      "   TEST 5: Mixed Edge Case -> PASS\n",
      "\n",
      "   ALL SYSTEM TESTS PASSED.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T14:34:17.305005Z",
     "start_time": "2026-01-01T14:34:17.288832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------------------------------------\n",
    "# DEEP SCAN: Audit the Final Data\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"   DEEP SCANNING PROCESSED DATA...\")\n",
    "\n",
    "# Define Forbidden Characters (Regex)\n",
    "forbidden_patterns = {\n",
    "    \"Arabic Y (ÙŠ)\": r'ÙŠ',\n",
    "    \"Arabic K (Ùƒ)\": r'Ùƒ',\n",
    "    \"Arabic Diacritics\": r'[\\u064B-\\u065F]',\n",
    "    \"Zero Width Joiner\": r'\\u200c',\n",
    "    \"Double Spaces\": r'\\s{2,}'\n",
    "}\n",
    "\n",
    "# Check the 'clean_professor_name_raw' column\n",
    "target_col = 'clean_professor_name_raw'\n",
    "scan_failed = False\n",
    "\n",
    "for name, pattern in forbidden_patterns.items():\n",
    "    # Find rows that match the forbidden pattern\n",
    "    bad_rows = df[df[target_col].str.contains(pattern, regex=True, na=False)]\n",
    "    \n",
    "    if len(bad_rows) > 0:\n",
    "        scan_failed = True\n",
    "        print(f\"   FAILURE: Found {len(bad_rows)} rows with {name}\")\n",
    "        print(f\"   Sample: {bad_rows[target_col].iloc[0]}\")\n",
    "    else:\n",
    "        print(f\"   PASSED: No '{name}' found.\")\n",
    "\n",
    "if not scan_failed:\n",
    "    print(\"\\n   CERTIFIED CLEAN: The data contains no forbidden characters.\")\n",
    "else:\n",
    "    print(\"\\n   ALERT: The data requires further cleaning.\")"
   ],
   "id": "c969d284b129961",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "   DEEP SCANNING PROCESSED DATA...\n",
      "   PASSED: No 'Arabic Y (ÙŠ)' found.\n",
      "   PASSED: No 'Arabic K (Ùƒ)' found.\n",
      "   PASSED: No 'Arabic Diacritics' found.\n",
      "   PASSED: No 'Zero Width Joiner' found.\n",
      "   PASSED: No 'Double Spaces' found.\n",
      "\n",
      "   CERTIFIED CLEAN: The data contains no forbidden characters.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T14:43:46.208093Z",
     "start_time": "2026-01-01T14:43:46.195828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 2: Remove Honorifics (Titles)\n",
    "# ---------------------------------------------------------\n",
    "import re\n",
    "\n",
    "def remove_titles(name):\n",
    "    if not isinstance(name, str): return \"\"\n",
    "    \n",
    "    titles = [\n",
    "        r'\\bØ¯Ú©ØªØ±\\b',   # Doctor\n",
    "        r'\\bØ§Ø³ØªØ§Ø¯\\b',  # Professor\n",
    "        r'\\bÙ…Ù‡Ù†Ø¯Ø³\\b',  # Engineer\n",
    "        r'\\bØ®Ø§Ù†Ù…\\b',   # Ms./Mrs.\n",
    "        r'\\bØ¢Ù‚Ø§ÛŒ\\b',   # Mr.\n",
    "        r'\\bØ­Ø§Ø¬\\b'     # Haji (Usually a title, safe to remove if separate word)\n",
    "    ]\n",
    "    \n",
    "    clean_name = name\n",
    "    for title in titles:\n",
    "        # Replace title with empty string\n",
    "        clean_name = re.sub(title, '', clean_name)\n",
    "    \n",
    "    return clean_name.strip()\n",
    "\n",
    "# --- UNIT TESTS (Updated for constraints) ---\n",
    "\n",
    "# 1. Standard Title Removal\n",
    "assert remove_titles(\"Ø¯Ú©ØªØ± Ø¹Ù„ÛŒ Ø±Ø¶Ø§ÛŒÛŒ\") == \"Ø¹Ù„ÛŒ Ø±Ø¶Ø§ÛŒÛŒ\"\n",
    "assert remove_titles(\"Ù…Ù‡Ù†Ø¯Ø³ Ù…Ø­Ù…Ø¯ÛŒ\") == \"Ù…Ø­Ù…Ø¯ÛŒ\"\n",
    "\n",
    "# 2. \"Seyed\" Preservation Tests (CRITICAL)\n",
    "assert remove_titles(\"Ø³ÛŒØ¯ Ø¹Ù„ÛŒ Ø­Ø³ÛŒÙ†ÛŒ\") == \"Ø³ÛŒØ¯ Ø¹Ù„ÛŒ Ø­Ø³ÛŒÙ†ÛŒ\"\n",
    "assert remove_titles(\"Ø®Ø§Ù†Ù… Ø³ÛŒØ¯Ù‡ Ø²Ù‡Ø±Ø§\") == \"Ø³ÛŒØ¯Ù‡ Ø²Ù‡Ø±Ø§\"\n",
    "\n",
    "# 3. Edge Cases\n",
    "assert remove_titles(\"Ø§Ø³ØªØ§Ø¯ Ø³ÛŒØ¯ Ù…Ø­Ù…Ø¯\") == \"Ø³ÛŒØ¯ Ù…Ø­Ù…Ø¯\"\n",
    "\n",
    "print(\"Title Remover Tests Passed.\")"
   ],
   "id": "b0701b78e9699549",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title Remover Tests Passed.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T14:46:03.384607Z",
     "start_time": "2026-01-01T14:46:03.341672Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------------------------------------\n",
    "# EXECUTION: Apply Corrected Title Removal\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Create the 'name_no_title' column\n",
    "df['name_no_title'] = df['clean_professor_name_raw'].apply(remove_titles)\n",
    "\n",
    "# Show a sample of what actually changed\n",
    "# This helps us confirm that \"Dr. Ali\" -> \"Ali\" happened, but \"Seyed Ali\" stayed \"Seyed Ali\"\n",
    "changed_rows = df[df['clean_professor_name_raw'] != df['name_no_title']]\n",
    "\n",
    "print(f\"Titles removed. {len(changed_rows)} names were simplified.\")\n",
    "\n",
    "if len(changed_rows) > 0:\n",
    "    print(\"\\nSample Changes:\")\n",
    "    display(changed_rows[['clean_professor_name_raw', 'name_no_title']].drop_duplicates().head(10))"
   ],
   "id": "cc25efeb653563e4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titles removed. 183 names were simplified.\n",
      "\n",
      "Sample Changes:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "     clean_professor_name_raw        name_no_title\n",
       "1            Ø¯Ú©ØªØ± Ù‡Ø§Ø¬Ø± ÙÙ„Ø§Ø­ØªÛŒ          Ù‡Ø§Ø¬Ø± ÙÙ„Ø§Ø­ØªÛŒ\n",
       "17           Ø¯Ú©ØªØ± Ø±Ø§Ù…ÛŒÙ† Ù‡Ø§Ø´Ù…ÛŒ          Ø±Ø§Ù…ÛŒÙ† Ù‡Ø§Ø´Ù…ÛŒ\n",
       "93          Ø¯Ú©ØªØ± Ø´Ø§Ù‡Ø±Ø® ÙØ±Ù‡Ù…Ù†Ø¯         Ø´Ø§Ù‡Ø±Ø® ÙØ±Ù‡Ù…Ù†Ø¯\n",
       "94       Ø¯Ú©ØªØ± Ø¯Ø§ÙˆØ¯ Ø¹Ø±Ø¨ Ø®Ø§Ø¨ÙˆØ±ÛŒ      Ø¯Ø§ÙˆØ¯ Ø¹Ø±Ø¨ Ø®Ø§Ø¨ÙˆØ±ÛŒ\n",
       "95               Ø¯Ú©ØªØ± Ø¹Ù„ÛŒ ØµØ¯Ø±              Ø¹Ù„ÛŒ ØµØ¯Ø±\n",
       "96       Ø¯Ú©ØªØ± Ø³ÛŒØ¯ Ù…Ø­Ù…Ø¯ Ø´Ù‡Ø±ØªØ§Ø´      Ø³ÛŒØ¯ Ù…Ø­Ù…Ø¯ Ø´Ù‡Ø±ØªØ§Ø´\n",
       "98   Ø¯Ú©ØªØ± Ø³ÛŒØ¯ Ø§Ø¯ÛŒØ¨ Ø§Ø¨Ø±ÛŒØ´Ù…ÛŒ ÙØ±  Ø³ÛŒØ¯ Ø§Ø¯ÛŒØ¨ Ø§Ø¨Ø±ÛŒØ´Ù…ÛŒ ÙØ±\n",
       "105             Ø¯Ú©ØªØ± Ø¹Ø¨Ø§Ø¯Ø§Ù„Ù‡ÛŒ             Ø¹Ø¨Ø§Ø¯Ø§Ù„Ù‡ÛŒ\n",
       "144          Ø¯Ú©ØªØ± Ø§Ù„Ù‡Ø§Ù… ÙØªØ§Ø­ÛŒ          Ø§Ù„Ù‡Ø§Ù… ÙØªØ§Ø­ÛŒ\n",
       "367            Ø¯Ú©ØªØ± Ø±Ø¶Ø§ Ø§Ø­Ù…Ø¯ÛŒ            Ø±Ø¶Ø§ Ø§Ø­Ù…Ø¯ÛŒ"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_professor_name_raw</th>\n",
       "      <th>name_no_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ø¯Ú©ØªØ± Ù‡Ø§Ø¬Ø± ÙÙ„Ø§Ø­ØªÛŒ</td>\n",
       "      <td>Ù‡Ø§Ø¬Ø± ÙÙ„Ø§Ø­ØªÛŒ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Ø¯Ú©ØªØ± Ø±Ø§Ù…ÛŒÙ† Ù‡Ø§Ø´Ù…ÛŒ</td>\n",
       "      <td>Ø±Ø§Ù…ÛŒÙ† Ù‡Ø§Ø´Ù…ÛŒ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Ø¯Ú©ØªØ± Ø´Ø§Ù‡Ø±Ø® ÙØ±Ù‡Ù…Ù†Ø¯</td>\n",
       "      <td>Ø´Ø§Ù‡Ø±Ø® ÙØ±Ù‡Ù…Ù†Ø¯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Ø¯Ú©ØªØ± Ø¯Ø§ÙˆØ¯ Ø¹Ø±Ø¨ Ø®Ø§Ø¨ÙˆØ±ÛŒ</td>\n",
       "      <td>Ø¯Ø§ÙˆØ¯ Ø¹Ø±Ø¨ Ø®Ø§Ø¨ÙˆØ±ÛŒ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Ø¯Ú©ØªØ± Ø¹Ù„ÛŒ ØµØ¯Ø±</td>\n",
       "      <td>Ø¹Ù„ÛŒ ØµØ¯Ø±</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Ø¯Ú©ØªØ± Ø³ÛŒØ¯ Ù…Ø­Ù…Ø¯ Ø´Ù‡Ø±ØªØ§Ø´</td>\n",
       "      <td>Ø³ÛŒØ¯ Ù…Ø­Ù…Ø¯ Ø´Ù‡Ø±ØªØ§Ø´</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Ø¯Ú©ØªØ± Ø³ÛŒØ¯ Ø§Ø¯ÛŒØ¨ Ø§Ø¨Ø±ÛŒØ´Ù…ÛŒ ÙØ±</td>\n",
       "      <td>Ø³ÛŒØ¯ Ø§Ø¯ÛŒØ¨ Ø§Ø¨Ø±ÛŒØ´Ù…ÛŒ ÙØ±</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Ø¯Ú©ØªØ± Ø¹Ø¨Ø§Ø¯Ø§Ù„Ù‡ÛŒ</td>\n",
       "      <td>Ø¹Ø¨Ø§Ø¯Ø§Ù„Ù‡ÛŒ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Ø¯Ú©ØªØ± Ø§Ù„Ù‡Ø§Ù… ÙØªØ§Ø­ÛŒ</td>\n",
       "      <td>Ø§Ù„Ù‡Ø§Ù… ÙØªØ§Ø­ÛŒ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>Ø¯Ú©ØªØ± Ø±Ø¶Ø§ Ø§Ø­Ù…Ø¯ÛŒ</td>\n",
       "      <td>Ø±Ø¶Ø§ Ø§Ø­Ù…Ø¯ÛŒ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T15:03:19.080195Z",
     "start_time": "2026-01-01T15:03:19.057932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from fuzzywuzzy import process, fuzz\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. IMPROVED ALGORITHM: Fuzzy + Subset Logic\n",
    "# ---------------------------------------------------------\n",
    "def get_standardized_names(df, similarity_threshold=85):\n",
    "    name_mapping = {}\n",
    "    \n",
    "    # Handle missing dept case for testing\n",
    "    if 'department' in df.columns:\n",
    "        departments = df['department'].unique()\n",
    "    else:\n",
    "        departments = [None]\n",
    "    \n",
    "    for dept in departments:\n",
    "        # Get names for this group\n",
    "        if dept is not None:\n",
    "            dept_names = df[df['department'] == dept]['name_no_title'].unique()\n",
    "        else:\n",
    "            dept_names = df['name_no_title'].unique()\n",
    "        \n",
    "        # Sort by length (Longest is Standard)\n",
    "        dept_names = sorted(dept_names, key=len, reverse=True)\n",
    "        \n",
    "        processed_in_dept = set()\n",
    "        \n",
    "        for name in dept_names:\n",
    "            if name in processed_in_dept:\n",
    "                continue\n",
    "            \n",
    "            standard = name\n",
    "            name_mapping[name] = standard\n",
    "            processed_in_dept.add(name)\n",
    "            \n",
    "            # Check remaining candidates\n",
    "            candidates = [n for n in dept_names if n not in processed_in_dept]\n",
    "            \n",
    "            for candidate in candidates:\n",
    "                # CRITERIA 1: Fuzzy Match (Strict)\n",
    "                # usage of token_sort_ratio prevents \"Ali Mohammadi\" merging with \"Hassan Mohammadi\"\n",
    "                fuzzy_score = fuzz.token_sort_ratio(standard, candidate)\n",
    "                \n",
    "                # CRITERIA 2: Subset Match (The \"Rezaei\" Fix)\n",
    "                # Checks if \"Rezaei\" is strictly inside \"Alireza Rezaei\" based on words\n",
    "                std_tokens = set(standard.split())\n",
    "                cand_tokens = set(candidate.split())\n",
    "                is_subset = cand_tokens.issubset(std_tokens)\n",
    "                \n",
    "                if fuzzy_score >= similarity_threshold or is_subset:\n",
    "                    name_mapping[candidate] = standard\n",
    "                    processed_in_dept.add(candidate)\n",
    "                    \n",
    "    return name_mapping\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. VERIFICATION: Run the Test Suite Again\n",
    "# ---------------------------------------------------------\n",
    "def run_fuzzy_tests():\n",
    "    \n",
    "    test_df = pd.DataFrame({\n",
    "        'department': ['Math', 'Math', 'Physics', 'Physics', 'Arts', 'Arts'],\n",
    "        'name_no_title': [\n",
    "            'Ø¹Ù„ÛŒØ±Ø¶Ø§ Ø±Ø¶Ø§ÛŒÛŒ', 'Ø±Ø¶Ø§ÛŒÛŒ',         # Case 1: Substring (Subset Logic)\n",
    "            'Ø¹Ù„ÛŒ Ù…Ø­Ù…Ø¯ÛŒ', 'Ø­Ø³Ù† Ù…Ø­Ù…Ø¯ÛŒ',        # Case 2: Distinct IDs (Fuzzy Logic Safety)\n",
    "            'Ø§Ø­Ù…Ø¯ÛŒ Ù†Ú˜Ø§Ø¯', 'Ù†Ú˜Ø§Ø¯ Ø§Ø­Ù…Ø¯ÛŒ'       # Case 3: Reordered (Fuzzy Logic Flexibility)\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    mapping = get_standardized_names(test_df, similarity_threshold=85)\n",
    "    \n",
    "    # TEST 1: The one that failed before\n",
    "    if mapping.get('Ø±Ø¶Ø§ÛŒÛŒ') == 'Ø¹Ù„ÛŒØ±Ø¶Ø§ Ø±Ø¶Ø§ÛŒÛŒ':\n",
    "        print(\"TEST 1: Subset Match (Rezaei -> Alireza Rezaei) -> PASS\")\n",
    "    else:\n",
    "        print(f\"TEST 1 FAIL: Got {mapping.get('Ø±Ø¶Ø§ÛŒÛŒ')}\")\n",
    "\n",
    "    # TEST 2: The safety check\n",
    "    if mapping.get('Ø­Ø³Ù† Ù…Ø­Ù…Ø¯ÛŒ') == 'Ø­Ø³Ù† Ù…Ø­Ù…Ø¯ÛŒ':\n",
    "        print(\"TEST 2: Distinct People Safety (Ali vs Hasan) -> PASS\")\n",
    "    else:\n",
    "        print(f\"TEST 2 FAIL: Merged incorrectly!\")\n",
    "\n",
    "    # TEST 3: Order check\n",
    "    if mapping.get('Ù†Ú˜Ø§Ø¯ Ø§Ø­Ù…Ø¯ÛŒ') == 'Ø§Ø­Ù…Ø¯ÛŒ Ù†Ú˜Ø§Ø¯':\n",
    "        print(\"TEST 3: Word Order Match -> PASS\")\n",
    "    else:\n",
    "        print(f\"TEST 3 FAIL: Order check failed\")\n",
    "\n",
    "    print(\"\\nALL LOGIC CHECKS PASSED.\")\n",
    "\n",
    "run_fuzzy_tests()"
   ],
   "id": "e849b97f473eb1de",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST 1: Subset Match (Rezaei -> Alireza Rezaei) -> PASS\n",
      "TEST 2: Distinct People Safety (Ali vs Hasan) -> PASS\n",
      "TEST 3: Word Order Match -> PASS\n",
      "\n",
      "ALL LOGIC CHECKS PASSED.\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T15:13:30.326235Z",
     "start_time": "2026-01-01T15:13:30.308520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def check_risk_pairs_pretty():\n",
    "    print(\"âš ï¸ DANGER ZONE TESTING (THRESHOLD = 85)...\\n\")\n",
    "    \n",
    "    pairs = [\n",
    "        # Case 1: Ali vs Alireza (Should NOT merge)\n",
    "        (\"Ø¹Ù„ÛŒØ±Ø¶Ø§ Ø±Ø¶Ø§ÛŒÛŒ\", \"Ø¹Ù„ÛŒ Ø±Ø¶Ø§ÛŒÛŒ\"),\n",
    "        \n",
    "        # Case 2: Reza vs Alireza (Should NOT merge)\n",
    "        (\"Ø¹Ù„ÛŒØ±Ø¶Ø§ Ø±Ø¶Ø§ÛŒÛŒ\", \"Ø±Ø¶Ø§ Ø±Ø¶Ø§ÛŒÛŒ\"),\n",
    "        \n",
    "        # Case 3: Same First Name, Diff Last Name (Should NOT merge)\n",
    "        (\"Ø¹Ù„ÛŒ Ù…Ø­Ù…Ø¯ÛŒ\", \"Ø¹Ù„ÛŒ Ø§Ø­Ù…Ø¯ÛŒ\"),\n",
    "        \n",
    "        # Case 4: The Valid Merge (Rezaei -> Alireza Rezaei)\n",
    "        (\"Ø¹Ù„ÛŒØ±Ø¶Ø§ Ø±Ø¶Ø§ÛŒÛŒ\", \"Ø±Ø¶Ø§ÛŒÛŒ\")\n",
    "    ]\n",
    "    \n",
    "    threshold = 85\n",
    "    results = []\n",
    "    \n",
    "    for name_a, name_b in pairs:\n",
    "        # Calculate strict fuzzy score\n",
    "        score = fuzz.token_sort_ratio(name_a, name_b)\n",
    "        \n",
    "        # Calculate Subset Logic\n",
    "        tokens_a = set(name_a.split())\n",
    "        tokens_b = set(name_b.split())\n",
    "        is_subset = tokens_b.issubset(tokens_a)\n",
    "        \n",
    "        # Decision Logic\n",
    "        if is_subset:\n",
    "            action = \"MERGE (Subset)\"\n",
    "        elif score >= threshold:\n",
    "            action = \"MERGE (Fuzzy)\" # <--- Risk Factor\n",
    "        else:\n",
    "            action = \"KEEP SEPARATE\"\n",
    "            \n",
    "        results.append({\n",
    "            'Name A (Standard)': name_a,\n",
    "            'Name B (Candidate)': name_b,\n",
    "            'Score': score,\n",
    "            'Is Subset?': is_subset,\n",
    "            'Action': action\n",
    "        })\n",
    "    \n",
    "    # Return pretty table\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run and display\n",
    "display(check_risk_pairs_pretty())"
   ],
   "id": "8f99cf804bed7885",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ DANGER ZONE TESTING (THRESHOLD = 85)...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  Name A (Standard) Name B (Candidate)  Score  Is Subset?          Action\n",
       "0      Ø¹Ù„ÛŒØ±Ø¶Ø§ Ø±Ø¶Ø§ÛŒÛŒ          Ø¹Ù„ÛŒ Ø±Ø¶Ø§ÛŒÛŒ     86       False   MERGE (Fuzzy)\n",
       "1      Ø¹Ù„ÛŒØ±Ø¶Ø§ Ø±Ø¶Ø§ÛŒÛŒ          Ø±Ø¶Ø§ Ø±Ø¶Ø§ÛŒÛŒ     67       False   KEEP SEPARATE\n",
       "2         Ø¹Ù„ÛŒ Ù…Ø­Ù…Ø¯ÛŒ          Ø¹Ù„ÛŒ Ø§Ø­Ù…Ø¯ÛŒ     44       False   KEEP SEPARATE\n",
       "3      Ø¹Ù„ÛŒØ±Ø¶Ø§ Ø±Ø¶Ø§ÛŒÛŒ              Ø±Ø¶Ø§ÛŒÛŒ     59        True  MERGE (Subset)"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name A (Standard)</th>\n",
       "      <th>Name B (Candidate)</th>\n",
       "      <th>Score</th>\n",
       "      <th>Is Subset?</th>\n",
       "      <th>Action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ø¹Ù„ÛŒØ±Ø¶Ø§ Ø±Ø¶Ø§ÛŒÛŒ</td>\n",
       "      <td>Ø¹Ù„ÛŒ Ø±Ø¶Ø§ÛŒÛŒ</td>\n",
       "      <td>86</td>\n",
       "      <td>False</td>\n",
       "      <td>MERGE (Fuzzy)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ø¹Ù„ÛŒØ±Ø¶Ø§ Ø±Ø¶Ø§ÛŒÛŒ</td>\n",
       "      <td>Ø±Ø¶Ø§ Ø±Ø¶Ø§ÛŒÛŒ</td>\n",
       "      <td>67</td>\n",
       "      <td>False</td>\n",
       "      <td>KEEP SEPARATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ø¹Ù„ÛŒ Ù…Ø­Ù…Ø¯ÛŒ</td>\n",
       "      <td>Ø¹Ù„ÛŒ Ø§Ø­Ù…Ø¯ÛŒ</td>\n",
       "      <td>44</td>\n",
       "      <td>False</td>\n",
       "      <td>KEEP SEPARATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ø¹Ù„ÛŒØ±Ø¶Ø§ Ø±Ø¶Ø§ÛŒÛŒ</td>\n",
       "      <td>Ø±Ø¶Ø§ÛŒÛŒ</td>\n",
       "      <td>59</td>\n",
       "      <td>True</td>\n",
       "      <td>MERGE (Subset)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T15:11:51.387784Z",
     "start_time": "2026-01-01T15:11:51.362080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------------------------------------\n",
    "# DANGER ZONE TEST (Clean Output)\n",
    "# ---------------------------------------------------------\n",
    "def check_risk_pairs_pretty():\n",
    "    print(\"ğŸ›¡ï¸ DANGER ZONE TESTING (THRESHOLD = 90)...\\n\")\n",
    "    \n",
    "    pairs = [\n",
    "        (\"Ø¹Ù„ÛŒØ±Ø¶Ø§ Ø±Ø¶Ø§ÛŒÛŒ\", \"Ø¹Ù„ÛŒ Ø±Ø¶Ø§ÛŒÛŒ\"),  # The problem case\n",
    "        (\"Ø¹Ù„ÛŒØ±Ø¶Ø§ Ø±Ø¶Ø§ÛŒÛŒ\", \"Ø±Ø¶Ø§ Ø±Ø¶Ø§ÛŒÛŒ\"),\n",
    "        (\"Ø¹Ù„ÛŒ Ù…Ø­Ù…Ø¯ÛŒ\", \"Ø¹Ù„ÛŒ Ø§Ø­Ù…Ø¯ÛŒ\"),\n",
    "        (\"Ø¹Ù„ÛŒØ±Ø¶Ø§ Ø±Ø¶Ø§ÛŒÛŒ\", \"Ø±Ø¶Ø§ÛŒÛŒ\")       # The subset case\n",
    "    ]\n",
    "    \n",
    "    THRESHOLD = 90\n",
    "    results = []\n",
    "    \n",
    "    for name_a, name_b in pairs:\n",
    "        score = fuzz.token_sort_ratio(name_a, name_b)\n",
    "        \n",
    "        # Calculate Subset Logic\n",
    "        tokens_a = set(name_a.split())\n",
    "        tokens_b = set(name_b.split())\n",
    "        is_subset = tokens_b.issubset(tokens_a)\n",
    "        \n",
    "        if is_subset:\n",
    "            action = \"MERGE (Subset)\"\n",
    "        elif score >= THRESHOLD:\n",
    "            action = \"MERGE (Fuzzy)\"\n",
    "        else:\n",
    "            action = \"KEEP SEPARATE\"\n",
    "            \n",
    "        results.append({\n",
    "            'Name A (Standard)': name_a,\n",
    "            'Name B (Candidate)': name_b,\n",
    "            'Score': score,\n",
    "            'Is Subset?': is_subset,\n",
    "            'Final Action': action\n",
    "        })\n",
    "    \n",
    "    # Return as a Pandas DataFrame for nice formatting\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run and display\n",
    "display(check_risk_pairs_pretty())"
   ],
   "id": "c83025e70e6bdcd2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ›¡ï¸ DANGER ZONE TESTING (THRESHOLD = 90)...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  Name A (Standard) Name B (Candidate)  Score  Is Subset?    Final Action\n",
       "0      Ø¹Ù„ÛŒØ±Ø¶Ø§ Ø±Ø¶Ø§ÛŒÛŒ          Ø¹Ù„ÛŒ Ø±Ø¶Ø§ÛŒÛŒ     86       False   KEEP SEPARATE\n",
       "1      Ø¹Ù„ÛŒØ±Ø¶Ø§ Ø±Ø¶Ø§ÛŒÛŒ          Ø±Ø¶Ø§ Ø±Ø¶Ø§ÛŒÛŒ     67       False   KEEP SEPARATE\n",
       "2         Ø¹Ù„ÛŒ Ù…Ø­Ù…Ø¯ÛŒ          Ø¹Ù„ÛŒ Ø§Ø­Ù…Ø¯ÛŒ     44       False   KEEP SEPARATE\n",
       "3      Ø¹Ù„ÛŒØ±Ø¶Ø§ Ø±Ø¶Ø§ÛŒÛŒ              Ø±Ø¶Ø§ÛŒÛŒ     59        True  MERGE (Subset)"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name A (Standard)</th>\n",
       "      <th>Name B (Candidate)</th>\n",
       "      <th>Score</th>\n",
       "      <th>Is Subset?</th>\n",
       "      <th>Final Action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ø¹Ù„ÛŒØ±Ø¶Ø§ Ø±Ø¶Ø§ÛŒÛŒ</td>\n",
       "      <td>Ø¹Ù„ÛŒ Ø±Ø¶Ø§ÛŒÛŒ</td>\n",
       "      <td>86</td>\n",
       "      <td>False</td>\n",
       "      <td>KEEP SEPARATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ø¹Ù„ÛŒØ±Ø¶Ø§ Ø±Ø¶Ø§ÛŒÛŒ</td>\n",
       "      <td>Ø±Ø¶Ø§ Ø±Ø¶Ø§ÛŒÛŒ</td>\n",
       "      <td>67</td>\n",
       "      <td>False</td>\n",
       "      <td>KEEP SEPARATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ø¹Ù„ÛŒ Ù…Ø­Ù…Ø¯ÛŒ</td>\n",
       "      <td>Ø¹Ù„ÛŒ Ø§Ø­Ù…Ø¯ÛŒ</td>\n",
       "      <td>44</td>\n",
       "      <td>False</td>\n",
       "      <td>KEEP SEPARATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ø¹Ù„ÛŒØ±Ø¶Ø§ Ø±Ø¶Ø§ÛŒÛŒ</td>\n",
       "      <td>Ø±Ø¶Ø§ÛŒÛŒ</td>\n",
       "      <td>59</td>\n",
       "      <td>True</td>\n",
       "      <td>MERGE (Subset)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T15:16:19.984342Z",
     "start_time": "2026-01-01T15:16:19.943008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def check_ahmad_mohammad_risk():\n",
    "    \n",
    "    pairs = [\n",
    "        # The Case You Feared: Same Last Name, Similar First Names\n",
    "        (\"Ø§Ø­Ù…Ø¯ Ø±Ø¶Ø§ÛŒÛŒ\", \"Ù…Ø­Ù…Ø¯ Ø±Ø¶Ø§ÛŒÛŒ\"),\n",
    "        \n",
    "        # Another Common Mix-up: Hamid vs Mohammad\n",
    "        (\"Ø­Ù…ÛŒØ¯ Ø±Ø¶Ø§ÛŒÛŒ\", \"Ù…Ø­Ù…Ø¯ Ø±Ø¶Ø§ÛŒÛŒ\"),\n",
    "        \n",
    "        # A Real Typo (Should still merge)\n",
    "        (\"Ø¹Ù„ÛŒØ±Ø¶Ø§ Ø±Ø¶Ø§ÛŒÛŒ\", \"Ø¹Ù„ÛŒØ±Ø¶Ø§ Ø±Ø¶Ø§Ø¦ÛŒ\") \n",
    "    ]\n",
    "    \n",
    "    # Let's test against strict thresholds\n",
    "    thresholds = [90, 95, 96]\n",
    "    results = []\n",
    "    \n",
    "    for name_a, name_b in pairs:\n",
    "        score = fuzz.token_sort_ratio(name_a, name_b)\n",
    "        \n",
    "        # Subset logic (always active)\n",
    "        is_subset = set(name_b.split()).issubset(set(name_a.split()))\n",
    "        \n",
    "        for thresh in thresholds:\n",
    "            if is_subset:\n",
    "                action = \"MERGE (Subset)\"\n",
    "            elif score >= thresh:\n",
    "                action = f\"MERGE (Risk @ {thresh})\" \n",
    "            else:\n",
    "                action = \"KEEP SEPARATE\"\n",
    "            \n",
    "            results.append({\n",
    "                'Name Pair': f\"{name_a} vs {name_b}\",\n",
    "                'Score': score,\n",
    "                'Threshold': thresh,\n",
    "                'Action': action\n",
    "            })\n",
    "            \n",
    "    return pd.pivot_table(pd.DataFrame(results), values='Action', index=['Name Pair', 'Score'], columns=['Threshold'], aggfunc='first')\n",
    "\n",
    "display(check_ahmad_mohammad_risk())"
   ],
   "id": "baac6e723e184298",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Threshold                                          90             95  \\\n",
       "Name Pair                    Score                                     \n",
       "Ø§Ø­Ù…Ø¯ Ø±Ø¶Ø§ÛŒÛŒ vs Ù…Ø­Ù…Ø¯ Ø±Ø¶Ø§ÛŒÛŒ     50         KEEP SEPARATE  KEEP SEPARATE   \n",
       "Ø­Ù…ÛŒØ¯ Ø±Ø¶Ø§ÛŒÛŒ vs Ù…Ø­Ù…Ø¯ Ø±Ø¶Ø§ÛŒÛŒ     50         KEEP SEPARATE  KEEP SEPARATE   \n",
       "Ø¹Ù„ÛŒØ±Ø¶Ø§ Ø±Ø¶Ø§ÛŒÛŒ vs Ø¹Ù„ÛŒØ±Ø¶Ø§ Ø±Ø¶Ø§Ø¦ÛŒ 92     MERGE (Risk @ 90)  KEEP SEPARATE   \n",
       "\n",
       "Threshold                                      96  \n",
       "Name Pair                    Score                 \n",
       "Ø§Ø­Ù…Ø¯ Ø±Ø¶Ø§ÛŒÛŒ vs Ù…Ø­Ù…Ø¯ Ø±Ø¶Ø§ÛŒÛŒ     50     KEEP SEPARATE  \n",
       "Ø­Ù…ÛŒØ¯ Ø±Ø¶Ø§ÛŒÛŒ vs Ù…Ø­Ù…Ø¯ Ø±Ø¶Ø§ÛŒÛŒ     50     KEEP SEPARATE  \n",
       "Ø¹Ù„ÛŒØ±Ø¶Ø§ Ø±Ø¶Ø§ÛŒÛŒ vs Ø¹Ù„ÛŒØ±Ø¶Ø§ Ø±Ø¶Ø§Ø¦ÛŒ 92     KEEP SEPARATE  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Threshold</th>\n",
       "      <th>90</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Name Pair</th>\n",
       "      <th>Score</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ø§Ø­Ù…Ø¯ Ø±Ø¶Ø§ÛŒÛŒ vs Ù…Ø­Ù…Ø¯ Ø±Ø¶Ø§ÛŒÛŒ</th>\n",
       "      <th>50</th>\n",
       "      <td>KEEP SEPARATE</td>\n",
       "      <td>KEEP SEPARATE</td>\n",
       "      <td>KEEP SEPARATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ø­Ù…ÛŒØ¯ Ø±Ø¶Ø§ÛŒÛŒ vs Ù…Ø­Ù…Ø¯ Ø±Ø¶Ø§ÛŒÛŒ</th>\n",
       "      <th>50</th>\n",
       "      <td>KEEP SEPARATE</td>\n",
       "      <td>KEEP SEPARATE</td>\n",
       "      <td>KEEP SEPARATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ø¹Ù„ÛŒØ±Ø¶Ø§ Ø±Ø¶Ø§ÛŒÛŒ vs Ø¹Ù„ÛŒØ±Ø¶Ø§ Ø±Ø¶Ø§Ø¦ÛŒ</th>\n",
       "      <th>92</th>\n",
       "      <td>MERGE (Risk @ 90)</td>\n",
       "      <td>KEEP SEPARATE</td>\n",
       "      <td>KEEP SEPARATE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T15:18:38.980826Z",
     "start_time": "2026-01-01T15:18:38.631042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------------------------------------\n",
    "# EXECUTION: FINAL STANDARDIZATION (Threshold 90)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# We use 90 because it filters distinct people (Score ~86) \n",
    "# but catches valid typos like Hamza (Score ~92)\n",
    "name_map = get_standardized_names(df, similarity_threshold=90)\n",
    "\n",
    "# Apply mapping\n",
    "df['professor_name_standard'] = df['name_no_title'].map(name_map).fillna(df['name_no_title'])\n",
    "\n",
    "# Stats\n",
    "n_before = df['clean_professor_name_raw'].nunique()\n",
    "n_after = df['professor_name_standard'].nunique()\n",
    "\n",
    "print(f\"Standardization Complete.\")\n",
    "print(f\"Total Unique Names Before: {n_before}\")\n",
    "print(f\"Total Unique Names After:  {n_after}\")\n",
    "print(f\"Reduction: {n_before - n_after} duplicates merged.\")\n",
    "\n",
    "# Show verified merges\n",
    "merged = df[df['name_no_title'] != df['professor_name_standard']]\n",
    "if len(merged) > 0:\n",
    "    print(\"\\nFinal Verified Merges:\")\n",
    "    display(merged[['department', 'name_no_title', 'professor_name_standard']].drop_duplicates().head(15))"
   ],
   "id": "473ec7ee29a2814f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardization Complete.\n",
      "Total Unique Names Before: 1269\n",
      "Total Unique Names After:  782\n",
      "Reduction: 487 duplicates merged.\n",
      "\n",
      "Final Verified Merges:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "         department            name_no_title           professor_name_standard\n",
       "2       Ù…Ù‡Ù†Ø¯Ø³ÛŒ Ù…ÙˆØ§Ø¯                Ø¹Ù„Ù… Ø§Ù„Ù‡Ø¯ÛŒ                    Ø³Ù…ÛŒÙ‡ Ø¹Ù„Ù… Ø§Ù„Ù‡Ø¯ÛŒ\n",
       "4   Ù…Ù‡Ù†Ø¯Ø³ÛŒ Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±             ÙØ±ÛŒØ¨Ø±Ø² Ù‚Ø§Ø³Ù…ÛŒ            ÙØ±ÛŒØ¨Ø±Ø² Ù‚Ø§Ø³Ù…ÛŒ ÙÛŒØ¶ Ø§Ø¨Ø§Ø¯ÛŒ\n",
       "8               Ø¨Ø±Ù‚  Ø³ÛŒØ¯ Ø¹Ù„ÛŒØ±Ø¶Ø§ Ù†Ø¸Ø§Ù… Ø§Ù„Ø­Ø³ÛŒÙ†ÛŒ  Ø³ÛŒØ¯ Ø¹Ù„ÛŒØ±Ø¶Ø§ Ù†Ø¸Ø§Ù… Ø§Ù„Ø­Ø³ÛŒÙ†ÛŒ Ø¹Ø² Ø§Ø¨Ø§Ø¯ÛŒ\n",
       "11            ÙÛŒØ²ÛŒÚ©            Ù…Ø­Ù…Ø¯Ø±Ø¶Ø§ Ø²Ù…Ø§Ù†ÛŒ              Ù…Ø­Ù…Ø¯Ø±Ø¶Ø§ Ø²Ù…Ø§Ù†ÛŒ Ù…ÛŒÙ…ÛŒØ§Ù†\n",
       "15           Ù…Ú©Ø§Ù†ÛŒÚ©               Ø­Ù…ÛŒØ¯ ØµÙØ§Ø±ÛŒ                  Ø­Ù…ÛŒØ¯ ØµÙØ§Ø±ÛŒ Ù†Ø·Ù†Ø²ÛŒ\n",
       "20            Ø±ÛŒØ§Ø¶ÛŒ                Ø¹Ù„ÛŒ Ù¾Ø§Ø±Ø³Ø§                Ø¹Ù„ÛŒ Ù¾Ø§Ø±Ø³Ø§ Ø±ÛŒØ§Ø¶ÛŒ Ø¯Ùˆ\n",
       "22            Ø±ÛŒØ§Ø¶ÛŒ     Ù…Ø­Ø¨ÙˆØ¨Ù‡ Ù…ÙˆÙ„ÙˆÛŒ Ø¹Ø±Ø¨Ø´Ø§Ù‡ÛŒ         Ø³ÛŒØ¯Ù‡ Ù…Ø­Ø¨ÙˆØ¨Ù‡ Ù…ÙˆÙ„ÙˆÛŒ Ø¹Ø±Ø¨Ø´Ø§Ù‡ÛŒ\n",
       "38            Ø±ÛŒØ§Ø¶ÛŒ                Ø§Ø­Ù…Ø¯ ÙØ±ÙˆØº                     Ø§Ø­Ù…Ø¯ Ø±Ø¶Ø§ ÙØ±ÙˆØº\n",
       "40            Ù…Ø¹Ø§Ø±Ù               Ø­Ø³ÛŒÙ† Ø³Ø¬Ø§Ø¯ÛŒ                    Ø³ÛŒØ¯ Ø­Ø³ÛŒÙ† Ø³Ø¬Ø§Ø¯ÛŒ\n",
       "48            Ø±ÛŒØ§Ø¶ÛŒ           Ø§Ø³Ø¯Ø§Ù„Ù‡ Ø§Ù‚Ø§Ø¬Ø§Ù†ÛŒ                   Ø§Ø³Ø¯Ø§Ù„Ù„Ù‡ Ø§Ù‚Ø§Ø¬Ø§Ù†ÛŒ\n",
       "49            ÙÛŒØ²ÛŒÚ©          Ø±Ø¶ÙˆØ§Ù†Ù‡ Ø§Ù…Ø±Ø§Ù„Ù„Ù‡ÛŒ             Ø±Ø¶ÙˆØ§Ù†Ù‡ Ø§Ù…Ø±Ø§Ù„Ù„Ù‡ÛŒ Ø¨ÛŒÙˆÚ©ÛŒ\n",
       "50            Ù…Ø¹Ø§Ø±Ù              Ù†ÙˆØ±Ø§Ù„Ù‡ Ù‡Ø§Ø¯ÛŒ                      Ù†ÙˆØ±Ø§Ù„Ù„Ù‡ Ù‡Ø§Ø¯ÛŒ\n",
       "54            Ø±ÛŒØ§Ø¶ÛŒ         Ù…Ø­Ù…Ø¯Ù‡Ø§Ø¯ÛŒ Ø¹Ù„Ø§Ø¦ÛŒØ§Ù†                 Ù…Ø­Ù…Ø¯ Ù‡Ø§Ø¯ÛŒ Ø¹Ù„Ø§Ø¦ÛŒØ§Ù†\n",
       "73            Ù…Ø¹Ø§Ø±Ù                    Ù†ØµÛŒØ±ÛŒ                        Ù…Ø­Ø³Ù† Ù†ØµÛŒØ±ÛŒ\n",
       "79            Ù…Ø¹Ø§Ø±Ù                    Ù‚Ø§Ø³Ù…ÛŒ                   Ù…Ø­Ù…ÙˆØ¯ Ø±Ø¶Ø§ Ù‚Ø§Ø³Ù…ÛŒ"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>department</th>\n",
       "      <th>name_no_title</th>\n",
       "      <th>professor_name_standard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ù…Ù‡Ù†Ø¯Ø³ÛŒ Ù…ÙˆØ§Ø¯</td>\n",
       "      <td>Ø¹Ù„Ù… Ø§Ù„Ù‡Ø¯ÛŒ</td>\n",
       "      <td>Ø³Ù…ÛŒÙ‡ Ø¹Ù„Ù… Ø§Ù„Ù‡Ø¯ÛŒ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ù…Ù‡Ù†Ø¯Ø³ÛŒ Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±</td>\n",
       "      <td>ÙØ±ÛŒØ¨Ø±Ø² Ù‚Ø§Ø³Ù…ÛŒ</td>\n",
       "      <td>ÙØ±ÛŒØ¨Ø±Ø² Ù‚Ø§Ø³Ù…ÛŒ ÙÛŒØ¶ Ø§Ø¨Ø§Ø¯ÛŒ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ø¨Ø±Ù‚</td>\n",
       "      <td>Ø³ÛŒØ¯ Ø¹Ù„ÛŒØ±Ø¶Ø§ Ù†Ø¸Ø§Ù… Ø§Ù„Ø­Ø³ÛŒÙ†ÛŒ</td>\n",
       "      <td>Ø³ÛŒØ¯ Ø¹Ù„ÛŒØ±Ø¶Ø§ Ù†Ø¸Ø§Ù… Ø§Ù„Ø­Ø³ÛŒÙ†ÛŒ Ø¹Ø² Ø§Ø¨Ø§Ø¯ÛŒ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ÙÛŒØ²ÛŒÚ©</td>\n",
       "      <td>Ù…Ø­Ù…Ø¯Ø±Ø¶Ø§ Ø²Ù…Ø§Ù†ÛŒ</td>\n",
       "      <td>Ù…Ø­Ù…Ø¯Ø±Ø¶Ø§ Ø²Ù…Ø§Ù†ÛŒ Ù…ÛŒÙ…ÛŒØ§Ù†</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Ù…Ú©Ø§Ù†ÛŒÚ©</td>\n",
       "      <td>Ø­Ù…ÛŒØ¯ ØµÙØ§Ø±ÛŒ</td>\n",
       "      <td>Ø­Ù…ÛŒØ¯ ØµÙØ§Ø±ÛŒ Ù†Ø·Ù†Ø²ÛŒ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Ø±ÛŒØ§Ø¶ÛŒ</td>\n",
       "      <td>Ø¹Ù„ÛŒ Ù¾Ø§Ø±Ø³Ø§</td>\n",
       "      <td>Ø¹Ù„ÛŒ Ù¾Ø§Ø±Ø³Ø§ Ø±ÛŒØ§Ø¶ÛŒ Ø¯Ùˆ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Ø±ÛŒØ§Ø¶ÛŒ</td>\n",
       "      <td>Ù…Ø­Ø¨ÙˆØ¨Ù‡ Ù…ÙˆÙ„ÙˆÛŒ Ø¹Ø±Ø¨Ø´Ø§Ù‡ÛŒ</td>\n",
       "      <td>Ø³ÛŒØ¯Ù‡ Ù…Ø­Ø¨ÙˆØ¨Ù‡ Ù…ÙˆÙ„ÙˆÛŒ Ø¹Ø±Ø¨Ø´Ø§Ù‡ÛŒ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Ø±ÛŒØ§Ø¶ÛŒ</td>\n",
       "      <td>Ø§Ø­Ù…Ø¯ ÙØ±ÙˆØº</td>\n",
       "      <td>Ø§Ø­Ù…Ø¯ Ø±Ø¶Ø§ ÙØ±ÙˆØº</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Ù…Ø¹Ø§Ø±Ù</td>\n",
       "      <td>Ø­Ø³ÛŒÙ† Ø³Ø¬Ø§Ø¯ÛŒ</td>\n",
       "      <td>Ø³ÛŒØ¯ Ø­Ø³ÛŒÙ† Ø³Ø¬Ø§Ø¯ÛŒ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Ø±ÛŒØ§Ø¶ÛŒ</td>\n",
       "      <td>Ø§Ø³Ø¯Ø§Ù„Ù‡ Ø§Ù‚Ø§Ø¬Ø§Ù†ÛŒ</td>\n",
       "      <td>Ø§Ø³Ø¯Ø§Ù„Ù„Ù‡ Ø§Ù‚Ø§Ø¬Ø§Ù†ÛŒ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>ÙÛŒØ²ÛŒÚ©</td>\n",
       "      <td>Ø±Ø¶ÙˆØ§Ù†Ù‡ Ø§Ù…Ø±Ø§Ù„Ù„Ù‡ÛŒ</td>\n",
       "      <td>Ø±Ø¶ÙˆØ§Ù†Ù‡ Ø§Ù…Ø±Ø§Ù„Ù„Ù‡ÛŒ Ø¨ÛŒÙˆÚ©ÛŒ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Ù…Ø¹Ø§Ø±Ù</td>\n",
       "      <td>Ù†ÙˆØ±Ø§Ù„Ù‡ Ù‡Ø§Ø¯ÛŒ</td>\n",
       "      <td>Ù†ÙˆØ±Ø§Ù„Ù„Ù‡ Ù‡Ø§Ø¯ÛŒ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Ø±ÛŒØ§Ø¶ÛŒ</td>\n",
       "      <td>Ù…Ø­Ù…Ø¯Ù‡Ø§Ø¯ÛŒ Ø¹Ù„Ø§Ø¦ÛŒØ§Ù†</td>\n",
       "      <td>Ù…Ø­Ù…Ø¯ Ù‡Ø§Ø¯ÛŒ Ø¹Ù„Ø§Ø¦ÛŒØ§Ù†</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Ù…Ø¹Ø§Ø±Ù</td>\n",
       "      <td>Ù†ØµÛŒØ±ÛŒ</td>\n",
       "      <td>Ù…Ø­Ø³Ù† Ù†ØµÛŒØ±ÛŒ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Ù…Ø¹Ø§Ø±Ù</td>\n",
       "      <td>Ù‚Ø§Ø³Ù…ÛŒ</td>\n",
       "      <td>Ù…Ø­Ù…ÙˆØ¯ Ø±Ø¶Ø§ Ù‚Ø§Ø³Ù…ÛŒ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T15:39:53.264831Z",
     "start_time": "2026-01-01T15:39:53.208859Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "#            Robust Course Name Unification\n",
    "# ---------------------------------------------------------\n",
    "def normalize_course_names(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    \n",
    "    # 1. Basic Persian Normalization (your existing function)\n",
    "    text = normalize_persian(text)\n",
    "    \n",
    "    # 2. Digit Conversion (Persian/Arabic -> English)\n",
    "    persian_digits = \"Û°Û±Û²Û³Û´ÛµÛ¶Û·Û¸Û¹\"\n",
    "    arabic_digits = \"Ù Ù¡Ù¢Ù£Ù¤Ù¥Ù¦Ù§Ù¨Ù©\"\n",
    "    english_digits = \"0123456789\"\n",
    "    translation_table = str.maketrans(persian_digits + arabic_digits, english_digits * 2)\n",
    "    text = text.translate(translation_table)\n",
    "    \n",
    "    # 3. ROBUSTNESS: Remove common junk punctuation\n",
    "    # Replaces ( ) - _ with a space\n",
    "    text = re.sub(r'[()\\-â€“_]', ' ', text)\n",
    "    \n",
    "    # 4. ROBUSTNESS: Fix \"Glued\" Numbers (e.g., \"Math1\" -> \"Math 1\")\n",
    "    # Adds a space if a letter is immediately followed by a number\n",
    "    text = re.sub(r'([a-zA-Z\\u0600-\\u06FF])(\\d)', r'\\1 \\2', text)\n",
    "    \n",
    "    # 5. Final Cleanup (Remove double spaces created by step 3 & 4)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply to the 'course_name' column (Check your column name!)\n",
    "target_col = 'course_name'  # <--- Verify this name matches your dataframe\n",
    "\n",
    "if target_col in df.columns:\n",
    "    df['clean_course_name'] = df[target_col].apply(normalize_course_names)\n",
    "    \n",
    "    n_before = df[target_col].nunique()\n",
    "    n_after = df['clean_course_name'].nunique()\n",
    "    \n",
    "    print(f\"Course Names Unified.\")\n",
    "    print(f\"Unique Courses Before: {n_before}\")\n",
    "    print(f\"Unique Courses After:  {n_after}\")\n",
    "    print(f\"Reduction: {n_before - n_after} variations merged.\")\n",
    "    \n",
    "    # SHOW THE PROOF: List top 10 most frequent courses to spot check\n",
    "    print(\"\\nTop 10 Most Common Courses (After Cleaning):\")\n",
    "    print(df['clean_course_name'].value_counts().head(10))\n",
    "\n",
    "else:\n",
    "    print(f\"Error: Column '{target_col}' not found.\")"
   ],
   "id": "978abdb4af2d0bdf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Course Names Unified.\n",
      "Unique Courses Before: 1330\n",
      "Unique Courses After:  1079\n",
      "Reduction: 251 variations merged.\n",
      "\n",
      "Top 10 Most Common Courses (After Cleaning):\n",
      "clean_course_name\n",
      "Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ø¯ÛŒÙØ±Ø§Ù†Ø³ÛŒÙ„    212\n",
      "Ø±ÛŒØ§Ø¶ÛŒ 1              176\n",
      "Ø±ÛŒØ§Ø¶ÛŒ 2              158\n",
      "ÙÛŒØ²ÛŒÚ© 2              135\n",
      "ÙÛŒØ²ÛŒÚ© 1              124\n",
      "Ø§Ø³ØªØ§ØªÛŒÚ©               77\n",
      "ÙØ§Ø±Ø³ÛŒ                 77\n",
      "Ø§Ù†Ø¯ÛŒØ´Ù‡ 1              75\n",
      "Ø§Ø®Ù„Ø§Ù‚ Ø§Ø³Ù„Ø§Ù…ÛŒ          70\n",
      "ØªØ±Ø¨ÛŒØª Ø¨Ø¯Ù†ÛŒ            61\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T15:54:20.003506Z",
     "start_time": "2026-01-01T15:54:19.979320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------------------------------------\n",
    "# INSPECTION: View Most Common Descriptions (Corrected Columns)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Correct Column Names found in your dataframe\n",
    "GRADING_COL = 'grading_status_raw'\n",
    "ATTENDANCE_COL = 'attendance_status_raw'\n",
    "\n",
    "print(f\"Top 30 Most Frequent 'Grading' Comments:\")\n",
    "if GRADING_COL in df.columns:\n",
    "    # We display the index (the text) and the count\n",
    "    print(df[GRADING_COL].value_counts().head(30))\n",
    "else:\n",
    "    print(f\"Error: Still can't find '{GRADING_COL}'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
    "\n",
    "print(f\"Top 30 Most Frequent 'Attendance' Comments:\")\n",
    "if ATTENDANCE_COL in df.columns:\n",
    "    print(df[ATTENDANCE_COL].value_counts().head(30))\n",
    "else:\n",
    "    print(f\"Error: Still can't find '{ATTENDANCE_COL}'\")"
   ],
   "id": "7d52cba073ceefd1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 30 Most Frequent 'Grading' Comments:\n",
      "grading_status_raw\n",
      "Ù…Ù†ØµÙØ§Ù†Ù‡ Ùˆ Ù‡Ø±Ú†ÛŒ Ø®ÙˆØ¯Øª Ø¨Ú¯ÛŒØ±ÛŒ                                                        1682\n",
      "Ø¯Ø³Øª Ø¨Ø§Ø² Ùˆ Ø¨Ø§ Ø§Ø±ÙØ§Ù‚                                                               1271\n",
      "Ù†Ù…Ø±Ù‡ Ø®ÙˆØ¨ÛŒ Ù†Ù…ÛŒØ´Ù‡ Ø§Ø²Ø´ÙˆÙ† Ú¯Ø±ÙØª                                                       1166\n",
      "ÛŒØ§Ø¯Ù… Ù†ÛŒØ³Øª Ú†Ø¬ÙˆØ±ÛŒ Ø¨ÙˆØ¯                                                                44\n",
      "Ø±Ù†Ø¯ÙˆÙ…                                                                               4\n",
      "ØªØ§ Ø­Ø¯ÙˆØ¯ÛŒ                                                                            2\n",
      "Ø§ÙØªØ¶Ø§Ø­                                                                              2\n",
      "Ø±Ù†Ø¯ÙˆÙ… Ù†Ù…Ø±Ù‡ Ù…ÛŒØ¯Ù†                                                                     2\n",
      "Ø¨Ø³ÛŒØ§Ø±Ù…Ù†ØµÙØ§Ù†Ù‡ ÙˆÙ…Ù‡Ø±Ø¨Ø§Ù†Ø§Ù†Ù‡                                                             2\n",
      "Ø®ÛŒØ±                                                                                 2\n",
      "Ø®Ø¨ Ø§Ø³Ø§Ø³Ø§ Ù†Ù…Ø±Ù‡ Ø§ÛŒ Ú©Ù‡ Ù…ÛŒØ¯Ù† Ø·ÙˆØ±ÛŒÙ‡ Ú©Ù‡ Ø³Ø®Øª Ú¯ÛŒØ±Ù‡ Ù…Ù† Ø®ÙˆØ¯Ù… Ø­Ø³ Ú©Ø±Ø¯Ù… Ù†Ù…Ø±Ù… Ø¨Ø§ÛŒØ¯ Ø¨ÛŒØ´ØªØ± Ø´Ù‡       1\n",
      "Ù…Ø·Ù„ÙˆØ¨                                                                               1\n",
      "Ø¨Ù‡ Ù†Ø¸Ø± ÛŒÚ© Ù†Ú¯Ø§Ù‡ Ú©Ù„ÛŒ Ø¨Ù‡ Ø¨Ø±Ú¯Ù‡ Ù…ÛŒÙ†Ø¯Ø§Ø²Ù† Ùˆ Ù†Ù…Ø±Ø§Øª Ù…ÛŒØ¯Ù†                                     1\n",
      "Ù‡Ù†ÙˆØ² Ù†Ù…Ø±Ù‡Ø§Ø´ÙˆÙ† Ù†ÛŒÙˆÙ…Ø¯Ù‡                                                                1\n",
      "Ø§Ø­ØªÙ…Ø§Ù„Ø§ Ø±Ù†Ø¯ÙˆÙ… ÙˆÙ„ÛŒ Ø¯Ù‚ÛŒÙ‚ Ù†Ù…ÛŒØ¯ÙˆÙ†Ù…                                                      1\n",
      "Ø´ÛŒÙˆÙ‡ ÛŒ Ù†Ù…Ø±Ù‡ Ø¯Ù‡ÛŒ ØµØ­ÛŒØ­ÛŒ Ù†Ø¯Ø§Ø±Ù†Ø¯                                                        1\n",
      "ÛŒÚ©ÛŒ Ø¯Ùˆ Ù†Ù…Ø±Ù‡ Ú©Ù…ØªØ± Ø§Ø² Ø­Ù‚Øª ÙˆÙ„ÛŒ Ù¾Ø§ÛŒÛŒÙ† Ù‡Ù… Ù†Ù…ÛŒØ¯Ù‡                                          1\n",
      "Ù†Ù‡ Ú†Ù†Ø¯Ø§Ù† Ù…Ù†ØµÙØ§Ù†Ù‡                                                                    1\n",
      "Ø®ÛŒÙ„ÛŒ Ø³Ø®ØªÚ¯ÛŒØ±Ø§Ù†Ù‡ Ù†Ù…Ø±Ù‡ Ù…ÛŒ Ø¯Ù‡Ù†Ø¯                                                         1\n",
      "Ø¨Ù‡ ØµÙˆØ±Øª Ø±Ù†Ø¯ÙˆÙ… Ùˆ Ú©Ø´Ú©ÛŒ Ú©Ø´Ú©ÛŒ                                                           1\n",
      "Ú©Ø§Ù…Ù„Ø§ Ø±Ù†Ø¯ÙˆÙ… Ù†Ù…Ø±Ù‡ Ù…ÛŒØ¯Ù†                                                               1\n",
      "Ø³Ø®Øª Ù…ÛŒØ´Ù‡ Ù†Ù…Ø±Ù‡ Ø®ÙˆØ¨ Ú¯Ø±ÙØª                                                              1\n",
      "Ø¨Ø³ÛŒØ§Ø± Ø¨Ø¯ Ùˆ Ø´Ø§Ù†Ø³ÛŒ                                                                    1\n",
      "Ù‡Ø± ÙØ¹Ø§Ù„ÛŒØªÛŒ Ø¨Ù‡ Ù‡Ø± Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø§ÛŒ Ø§Ù†Ø¬Ø§Ù… Ø¨Ø¯Ù‡ÛŒØ¯ Û±Ûµ ÛŒØ§ Û±Û¶ Ù…ÛŒÚ¯ÛŒØ±ÛŒØ¯                             1\n",
      "Ø§Ø²Ø­Ù‚Øª Ø®ÛŒÙ„ÛŒ Ú©Ù…ØªØ±Ù…ÛŒØ¯Ù‡                                                                 1\n",
      "ØºÛŒØ± Ù…Ù†Ø·Ù‚ÛŒ Ù‡Ø³Øª                                                                       1\n",
      "Ø¯Ù‡ Ø¨ÛŒØ³Øª Ø³ÛŒ Ú†Ù‡Ù„ Ù…ÛŒÚ©Ù†Ù†Ø¯                                                               1\n",
      "Ø¨Ø±Ø§ÛŒ ØµØ¯Ù… Ø¨Ù‡ ØµØ¯Ù… Ù†Ù…Ø±Ù‡ Ù‡Ø§Øª Ø¨Ø§ÛŒØ¯ Ú©Ø§Ø± Ú©Ø±Ø¯Ù‡ Ø¨Ø§Ø´ÛŒ                                         1\n",
      "Ù†Ù…ÛŒ Ø¯Ø§Ù†Ù…                                                                            1\n",
      "Ø­Ù‚ØªÙˆ Ø¨Ù‡Øª Ù†Ù…ÛŒØ¯Ù‡ Ùˆ Ø§Ø² Ù†Ù…Ø±Ù‡ ÙˆØ§Ù‚Ø¹ÛŒ Ø®ÙˆØ¯Øª Ú©Ù…ØªØ± Ù…ÛŒØ¯Ù‡                                       1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "========================================\n",
      "\n",
      "Top 30 Most Frequent 'Attendance' Comments:\n",
      "attendance_status_raw\n",
      "Ø­Ø¶ÙˆØ± Ù…Ù‡Ù… Ø§Ø³Øª Ùˆ ØªØ§Ø«ÛŒØ± Ù…Ø³ØªÙ‚ÛŒÙ… Ø¯Ø§Ø±Ø¯     2211\n",
      "Ø­Ø¶ÙˆØ± Ù…Ù‡Ù… Ù†ÛŒØ³Øª Ø§Ù…Ø§ ØªØ§Ø«ÛŒØ± Ù…Ø«Ø¨Øª Ø¯Ø§Ø±Ø¯     973\n",
      "Ø­Ø¶ÙˆØ± Ùˆ ØºÛŒØ§Ø¨ Ù†Ù…ÛŒ Ú©Ù†Ø¯                   945\n",
      "ÛŒØ§Ø¯Ù… Ù†Ù…ÛŒØ§Ø¯                            147\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T16:17:14.302841Z",
     "start_time": "2026-01-01T16:17:14.277740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------------------------------------\n",
    "# TASK 4, PART 2: FINAL CLASSIFIER (Re-run Safe)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# We read from RAW columns (Safe to re-run this cell many times)\n",
    "GRADING_COL = 'grading_status_raw'\n",
    "ATTENDANCE_COL = 'attendance_status_raw'\n",
    "\n",
    "def categorize_grading(text):\n",
    "    if not isinstance(text, str): return \"Ù†Ø§Ù…Ø´Ø®Øµ\"\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 0. Unknowns\n",
    "    if any(k in text for k in ['ÛŒØ§Ø¯Ù… Ù†ÛŒØ³Øª', 'Ù†Ù…ÛŒØ¯ÙˆÙ†Ù…', 'Ù†ÛŒÙˆÙ…Ø¯Ù‡', 'Ø®ÛŒØ±', 'Ù†Ø¯Ø§Ø±Ù…']):\n",
    "        return \"Ù†Ø§Ù…Ø´Ø®Øµ\"\n",
    "\n",
    "    # 1. Easy (Ø¢Ø³Ø§Ù†)\n",
    "    if any(k in text for k in ['Ø¯Ø³Øª Ø¨Ø§Ø²', 'Ø§Ø±ÙØ§Ù‚', 'Ø®ÙˆØ¨ Ù†Ù…Ø±Ù‡', 'Ø¹Ø§Ù„ÛŒ', 'Û²Û°', '20', 'Ù‡Ù„Ùˆ', 'Ø®ÙˆØ´ Ù†Ù…Ø±Ù‡', 'Ù…Ù‡Ø±Ø¨Ø§Ù†', 'Ù…Ø·Ù„ÙˆØ¨']):\n",
    "        return \"Ø¢Ø³Ø§Ù†\"\n",
    "    \n",
    "    # 2. Strict / Negative (Ø³Ø®Øªâ€ŒÚ¯ÛŒØ±)\n",
    "    # CRITICAL FIX: We check 'ØºÛŒØ± Ù…Ù†ØµÙØ§Ù†Ù‡' and 'Ù†Ù‡ Ú†Ù†Ø¯Ø§Ù†' HERE so they don't get trapped as 'Fair' later.\n",
    "    strict_keywords = [\n",
    "        'Ù†Ù…ÛŒØ´Ù‡', 'Ù†Ù…ÛŒ Ø¯Ù‡', 'Ù†Ù…ÛŒØ¯Ù‡', 'Ø³Ø®Øª', 'Ø±Ù†Ø¯ÙˆÙ…', 'Ø´Ø§Ù†Ø³ÛŒ', 'Ø§ÙØªØ¶Ø§Ø­', \n",
    "        'Ú©Ù…ØªØ±', 'Ø¨Ø¯', 'Ø´Ø§Ù†Ø³', 'Ú©Ø´Ú©ÛŒ', 'ØºÛŒØ± Ù…Ù†Ø·Ù‚ÛŒ', 'Ø¯Ù‡ Ø¨ÛŒØ³Øª', \n",
    "        'Ù†Ù‡ Ú†Ù†Ø¯Ø§Ù†', 'ØºÛŒØ± Ù…Ù†ØµÙØ§Ù†Ù‡', 'ØµØ­ÛŒØ­ÛŒ Ù†Ø¯Ø§Ø±Ù†Ø¯', 'Ù¾Ø§ÛŒÛŒÙ†'\n",
    "    ]\n",
    "    if any(k in text for k in strict_keywords):\n",
    "        return \"Ø³Ø®Øªâ€ŒÚ¯ÛŒØ±\"\n",
    "    \n",
    "    # 3. Fair (Ù…Ù†ØµÙØ§Ù†Ù‡)\n",
    "    # Only runs if the negative checks above failed\n",
    "    if any(k in text for k in ['Ù…Ù†ØµÙØ§Ù†Ù‡', 'Ø­Ù‚', 'Ù‡Ø±Ú†ÛŒ', 'ÙˆØ§Ù‚Ø¹ÛŒ', 'Ù†Ø±Ù…Ø§Ù„']):\n",
    "        return \"Ù…Ù†ØµÙØ§Ù†Ù‡\"\n",
    "        \n",
    "    return \"Ù†Ø§Ù…Ø´Ø®Øµ\"\n",
    "\n",
    "def categorize_attendance(text):\n",
    "    if not isinstance(text, str): return \"Ù†Ø§Ù…Ø´Ø®Øµ\"\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 0. Unknowns\n",
    "    if any(k in text for k in ['ÛŒØ§Ø¯Ù… Ù†ÛŒØ³Øª', 'Ù†Ù…ÛŒØ¯ÙˆÙ†Ù…', 'ÛŒØ§Ø¯Ù… Ù†Ù…ÛŒØ§Ø¯']):\n",
    "        return \"Ù†Ø§Ù…Ø´Ø®Øµ\"\n",
    "\n",
    "    # 1. Strict (Ø³Ø®Øªâ€ŒÚ¯ÛŒØ±)\n",
    "    if any(k in text for k in ['Ù…Ù‡Ù… Ø§Ø³Øª', 'Ø§Ø¬Ø¨Ø§Ø±ÛŒ', 'Ú¯ÛŒØ±', 'Ù„ÛŒØ³Øª', 'ØªØ§Ø«ÛŒØ± Ù…Ø³ØªÙ‚ÛŒÙ…', 'Ù…ÛŒØ®ÙˆÙ†Ù‡']):\n",
    "        return \"Ø³Ø®Øªâ€ŒÚ¯ÛŒØ±\"\n",
    "    \n",
    "    # 2. Free (Ø¢Ø²Ø§Ø¯)\n",
    "    # Includes \"Positive effect\" (Ta'sir Mosbat) because usually that means attendance isn't mandatory\n",
    "    if any(k in text for k in ['Ù…Ù‡Ù… Ù†ÛŒØ³Øª', 'Ù†Ù…ÛŒ Ú©Ù†Ø¯', 'Ù†Ø¯Ø§Ø±Ø¯', 'Ø¢Ø²Ø§Ø¯', 'Ø§Ø®ØªÛŒØ§Ø±ÛŒ', 'ØªØ§Ø«ÛŒØ± Ù…Ø«Ø¨Øª']):\n",
    "        return \"Ø¢Ø²Ø§Ø¯\"\n",
    "    \n",
    "    # 3. Moderate (Ù…ØªÙˆØ³Ø·)\n",
    "    if any(k in text for k in ['Ú¯Ø§Ù‡ÛŒ', 'Ù…ØªÙˆØ³Ø·']):\n",
    "        return \"Ù…ØªÙˆØ³Ø·\"\n",
    "        \n",
    "    return \"Ù†Ø§Ù…Ø´Ø®Øµ\"\n",
    "\n",
    "# --- EXECUTION ---\n",
    "\n",
    "# Create/Overwrite the standard columns\n",
    "df['grading_standard'] = df[GRADING_COL].apply(categorize_grading)\n",
    "df['attendance_standard'] = df[ATTENDANCE_COL].apply(categorize_attendance)\n",
    "\n",
    "print(\"\\nFinal Grading Distribution:\")\n",
    "print(df['grading_standard'].value_counts())\n",
    "\n",
    "print(\"\\nFinal Attendance Distribution:\")\n",
    "print(df['attendance_standard'].value_counts())"
   ],
   "id": "a83051c6f6b4f4ec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Grading Distribution:\n",
      "grading_standard\n",
      "Ù…Ù†ØµÙØ§Ù†Ù‡    1687\n",
      "Ø¢Ø³Ø§Ù†       1280\n",
      "Ø³Ø®Øªâ€ŒÚ¯ÛŒØ±    1219\n",
      "Ù†Ø§Ù…Ø´Ø®Øµ      362\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Final Attendance Distribution:\n",
      "attendance_standard\n",
      "Ø³Ø®Øªâ€ŒÚ¯ÛŒØ±    2211\n",
      "Ø¢Ø²Ø§Ø¯       1918\n",
      "Ù†Ø§Ù…Ø´Ø®Øµ      419\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T16:20:30.884644Z",
     "start_time": "2026-01-01T16:20:30.783689Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------------------------------------\n",
    "#                 Save the Clean Data\n",
    "# ---------------------------------------------------------\n",
    "from pathlib import Path\n",
    "\n",
    "# Define output path\n",
    "output_path = Path(r'../data/processed/clean_data.csv')\n",
    "\n",
    "# Drop intermediate/raw columns to keep the file clean\n",
    "# We keep the new 'professor_name_standard', 'grading_standard', etc.\n",
    "cols_to_drop = [\n",
    "    'clean_professor_name_raw', 'name_no_title', \n",
    "    'clean_course_name', 'grading_status_raw', 'attendance_status_raw'\n",
    "]\n",
    "\n",
    "# Create the final dataframe\n",
    "final_df = df.drop(columns=[c for c in cols_to_drop if c in df.columns], errors='ignore')\n",
    "\n",
    "# Rename standard columns to be the main ones for Phase 3\n",
    "final_df = final_df.rename(columns={\n",
    "    'professor_name_standard': 'professor_name',\n",
    "    'grading_standard': 'grading_status',\n",
    "    'attendance_standard': 'attendance_status'\n",
    "})\n",
    "\n",
    "# Save\n",
    "final_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"SUCCESS: Phase 2 Clean data saved to {output_path}\")\n",
    "print(f\"Final Row Count: {len(final_df)}\")\n",
    "print(f\"Columns: {final_df.columns.tolist()}\")"
   ],
   "id": "5b06691be263f207",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: Phase 2 Clean data saved to ..\\data\\processed\\clean_data.csv\n",
      "Final Row Count: 4548\n",
      "Columns: ['id', 'date', 'professor_name_raw', 'department', 'course_name', 'term', 'comment_text', 'rating_1', 'rating_2', 'rating_3', 'rating_4', 'rating_5', 'rating_6', 'clean_department', 'clean_comment_text', 'professor_name', 'grading_status', 'attendance_status']\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a591cf81089a301e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
